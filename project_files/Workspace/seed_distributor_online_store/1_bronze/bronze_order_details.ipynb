{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339d8db8-9582-441e-9777-39f86196c7db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('/Workspace/Repos/zhastay_yeltay@epam.com/utils/'))\n",
    "\n",
    "from delta.tables import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from init import *\n",
    "init_spark()\n",
    "\n",
    "from util_logger import init_logger\n",
    "dbutils.widgets.text('task', \"test_logger\")\n",
    "logger = init_logger(dbutils.widgets.get('task'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32a270a9-33b2-4417-8bbb-2c2e924224b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## order_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37a2f12-8ed9-47a8-8736-51cfc9a85bb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3759760084217192>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m order_details_df_upd \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msource_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/orderDetails/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 3\u001B[0m order_details_df_raw \u001B[38;5;241m=\u001B[39m DeltaTable\u001B[38;5;241m.\u001B[39mforPath(spark, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbronze\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/order_details/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m order_details_df_raw\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morder_details\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmerge(\n",
       "\u001B[1;32m      6\u001B[0m     order_details_df_upd\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdates\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m      7\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m     }\n",
       "\u001B[1;32m     18\u001B[0m )\u001B[38;5;241m.\u001B[39mexecute()\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Removing old history\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/delta/tables.py:402\u001B[0m, in \u001B[0;36mDeltaTable.forPath\u001B[0;34m(cls, sparkSession, path, hadoopConf)\u001B[0m\n",
       "\u001B[1;32m    399\u001B[0m jvm: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJVMView\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m sparkSession\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n",
       "\u001B[1;32m    400\u001B[0m jsparkSession: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJavaObject\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m sparkSession\u001B[38;5;241m.\u001B[39m_jsparkSession  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n",
       "\u001B[0;32m--> 402\u001B[0m jdt \u001B[38;5;241m=\u001B[39m \u001B[43mjvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdelta\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtables\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDeltaTable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjsparkSession\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhadoopConf\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    403\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DeltaTable(sparkSession, jdt)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [DELTA_MISSING_DELTA_TABLE] `dbfs:/mnt/adls_custom/data/Team_B/zhastay_yeltay/01_bronze/order_details/` is not a Delta table."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[DELTA_MISSING_DELTA_TABLE] `dbfs:/mnt/adls_custom/data/Team_B/zhastay_yeltay/01_bronze/order_details/` is not a Delta table."
       },
       "metadata": {
        "errorSummary": "[DELTA_MISSING_DELTA_TABLE] `dbfs:/mnt/adls_custom/data/Team_B/zhastay_yeltay/01_bronze/order_details/` is not a Delta table. SQLSTATE: 42P01"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "DELTA_MISSING_DELTA_TABLE",
        "sqlState": "42P01",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-3759760084217192>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m order_details_df_upd \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msource_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/orderDetails/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m order_details_df_raw \u001B[38;5;241m=\u001B[39m DeltaTable\u001B[38;5;241m.\u001B[39mforPath(spark, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbronze\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/order_details/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m order_details_df_raw\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morder_details\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmerge(\n\u001B[1;32m      6\u001B[0m     order_details_df_upd\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdates\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     17\u001B[0m     }\n\u001B[1;32m     18\u001B[0m )\u001B[38;5;241m.\u001B[39mexecute()\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Removing old history\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/spark/python/delta/tables.py:402\u001B[0m, in \u001B[0;36mDeltaTable.forPath\u001B[0;34m(cls, sparkSession, path, hadoopConf)\u001B[0m\n\u001B[1;32m    399\u001B[0m jvm: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJVMView\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m sparkSession\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    400\u001B[0m jsparkSession: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJavaObject\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m sparkSession\u001B[38;5;241m.\u001B[39m_jsparkSession  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m--> 402\u001B[0m jdt \u001B[38;5;241m=\u001B[39m \u001B[43mjvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdelta\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtables\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDeltaTable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjsparkSession\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhadoopConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DeltaTable(sparkSession, jdt)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [DELTA_MISSING_DELTA_TABLE] `dbfs:/mnt/adls_custom/data/Team_B/zhastay_yeltay/01_bronze/order_details/` is not a Delta table."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    # Reading updated order details data\n",
    "    order_details_df_upd = spark.read.parquet(f\"{source_path}/orderDetails/\")\n",
    "    logger.info(\"Updated order details data read successfully from %s.\", f\"{source_path}/orderDetails/\")\n",
    "\n",
    "    # Accessing the raw Delta table\n",
    "    order_details_df_raw = DeltaTable.forPath(spark, f\"{bronze}/order_details/\")\n",
    "    logger.info(\"Accessed DeltaTable for raw order details at %s.\", f\"{bronze}/order_details/\")\n",
    "\n",
    "    # Performing merge operation\n",
    "    # Note: Only inserting not matched records\n",
    "    order_details_df_raw.alias(\"order_details\").merge(\n",
    "        order_details_df_upd.alias(\"updates\"),\n",
    "        \"\"\"\n",
    "            order_details.OrderId = updates.OrderId \n",
    "            AND order_details.ItemId = updates.ItemId \n",
    "            AND order_details.Quantity = updates.Quantity\n",
    "        \"\"\"\n",
    "    ).whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"OrderId\": \"updates.OrderId\",\n",
    "            \"ItemId\": \"updates.ItemId\",\n",
    "            \"Quantity\": \"updates.Quantity\",\n",
    "        }\n",
    "    ).execute()\n",
    "    logger.info(\"Merge operation for new records in order details executed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred during the order details data processing: %s\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "784a8e37-45a5-4b59-92bb-a939458995ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "1945317b-526c-43e1-8c53-95c26b5344d6",
     "origId": 328367940062598,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3759760084217194,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_order_details",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
