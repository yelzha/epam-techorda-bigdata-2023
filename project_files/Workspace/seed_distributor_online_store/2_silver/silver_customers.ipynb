{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7293ad6-32f1-4d54-a2fc-62ee700dc119",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('/Workspace/Repos/zhastay_yeltay@epam.com/utils/'))\n",
    "\n",
    "from delta.tables import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, DateType, LongType\n",
    "\n",
    "from init import *\n",
    "from udfs import * \n",
    "init_spark()\n",
    "\n",
    "from util_logger import init_logger\n",
    "dbutils.widgets.text('task', \"test_logger\")\n",
    "logger = init_logger(dbutils.widgets.get('task'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7abeb4cc-d4ad-4ea7-b138-2de9aeb0d3ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    # Log loading data from the bronze layer\n",
    "    logger.info(f\"Loading customer data from the Delta table at {bronze}/customers.\")\n",
    "    bronze_customers_df = spark.read.format(\"delta\").load(f\"{bronze}/customers\")\n",
    "\n",
    "    # Log start of data transformation\n",
    "    logger.info(\"Starting transformation of customer data.\")\n",
    "    silver_customers_df_upd = (\n",
    "        bronze_customers_df\n",
    "        .withColumnRenamed(\"CreatedOn\", \"created_on\")\n",
    "        .withColumn(\"type\", capitalize_udf(F.col(\"type\")))\n",
    "        .withColumn(\"status\", F.when(F.col('status') == 'VIP', F.col('status')).otherwise(capitalize_udf(F.col(\"status\"))))\n",
    "        .withColumn(\n",
    "            \"is_valid\",\n",
    "            F.when(\n",
    "                F.col(\"id\").isNotNull()\n",
    "                & F.col(\"type\").isNotNull()\n",
    "                & F.col(\"status\").isNotNull()\n",
    "                & F.col(\"created_on\").isNotNull()\n",
    "                & (F.col(\"type\").isin([\"Affiliate\", \"Individual\"]))\n",
    "                & (F.col(\"status\").isin([\"Regular\", \"VIP\"]))\n",
    "                & F.col(\"created_on\").between(\n",
    "                    F.lit(\"2000-01-01\").cast(TimestampType()), F.current_timestamp()\n",
    "                ),\n",
    "                True,\n",
    "            ).otherwise(False),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    has_invalid = silver_customers_df_upd.filter(~F.col('is_valid')).limit(1).count() > 0\n",
    "    logger.info(f\"Invalid entries found: {has_invalid}\")\n",
    "\n",
    "    # Log start of merge operation into the silver layer\n",
    "    logger.info(\"Starting merge operation for customer data into the Silver layer.\")\n",
    "    silver_customers_df = DeltaTable.forPath(spark, f\"{silver}/customers/\")\n",
    "    silver_customers_df.alias(\"customers\").merge(\n",
    "        silver_customers_df_upd.alias(\"updates\"), \"customers.id = updates.id\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\"\"\"\n",
    "                customers.type != updates.type\n",
    "                OR customers.status != updates.status\n",
    "                OR customers.created_on != updates.created_on\n",
    "            \"\"\",\n",
    "        set={\n",
    "            \"type\": \"updates.type\",\n",
    "            \"status\": \"updates.status\",\n",
    "            \"created_on\": \"updates.created_on\"\n",
    "        },\n",
    "    ).whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"id\": \"updates.id\",\n",
    "            \"type\": \"updates.type\",\n",
    "            \"status\": \"updates.status\",\n",
    "            \"created_on\": \"updates.created_on\"\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "    # Log the successful completion of the merge operation\n",
    "    logger.info(\"Merge operation completed successfully for customer data.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred during the processing and merging of customer data.\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "030beaf5-ea2a-4f8f-91a5-fe3cba150e8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1655979001493767,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_customers",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
