{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "307a5b6b-664e-4346-b5ee-c7076b4050cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: openpyxl in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ea3d0f3b-e47d-440b-97b8-42a7290b28e7/lib/python3.10/site-packages (3.1.2)\nRequirement already satisfied: et-xmlfile in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ea3d0f3b-e47d-440b-97b8-42a7290b28e7/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe8d056-e945-4e06-965f-c350801d345d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('/Workspace/Repos/zhastay_yeltay@epam.com/utils/'))\n",
    "\n",
    "from delta.tables import *\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from init import *\n",
    "init_spark()\n",
    "\n",
    "from util_logger import init_logger\n",
    "dbutils.widgets.text('task', \"test_logger\")\n",
    "logger = init_logger(dbutils.widgets.get('task'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "804853a0-091f-44f2-a845-6b1cae1df9c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Metropolitan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7591c63-8463-4b5e-8835-93364c9f6f24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:test_logger:Reading statistical areas data from Excel file at /dbfs/mnt/adls_custom/data/Team_B/zhastay_yeltay/zhastay_yeltay_01_bronze/statistical_areas.xlsx.\nINFO:test_logger:Converting pandas DataFrame to Spark DataFrame.\nINFO:test_logger:Starting cleaning and filtering of statistical areas data.\nINFO:test_logger:Statistical areas data prepared successfully for further processing.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Log the reading of Excel file into DataFrame\n",
    "    logger.info(f\"Reading statistical areas data from Excel file at /{working_path.replace(':', '')}/{schema_bronze_name}/statistical_areas.xlsx.\")\n",
    "    statistical_areas = pd.read_excel(\n",
    "        f\"/{working_path.replace(':', '')}/{schema_bronze_name}/statistical_areas.xlsx\", \n",
    "        skiprows=2, \n",
    "        skipfooter=3\n",
    "    )\n",
    "    statistical_areas.columns = ['metropolitan_code', 'metropolitan', 'statistical_category', 'city', 'state_code', 'place_code']\n",
    "\n",
    "    # Convert pandas DataFrame to Spark DataFrame\n",
    "    logger.info(\"Converting pandas DataFrame to Spark DataFrame.\")\n",
    "    bronze_statistical_areas_df = spark.createDataFrame(statistical_areas)\n",
    "\n",
    "    # Prepare regex pattern for cleaning city names\n",
    "    check_list = [\n",
    "        '(balance)',\n",
    "        'government',\n",
    "        'metropolitan',\n",
    "        'metro',\n",
    "        'unified',\n",
    "        'government',\n",
    "        'consolidated',\n",
    "        'County'\n",
    "    ]\n",
    "    pattern = '|'.join(re.escape(word) for word in set(check_list))\n",
    "\n",
    "    # Log the start of data cleaning and filtering\n",
    "    logger.info(\"Starting cleaning and filtering of statistical areas data.\")\n",
    "    silver_statistical_areas_df = (\n",
    "        bronze_statistical_areas_df\n",
    "        .withColumn(\"city\", F.trim(F.regexp_replace(\"city\", pattern, \"\")))\n",
    "        .withColumn(\"city\", F.regexp_replace(\"city\", \"Urban Honolulu\", \"Honolulu\"))\n",
    "        .filter(F.col('statistical_category') == 'Metropolitan Statistical Area')\n",
    "    )\n",
    "\n",
    "    # Log successful data preparation\n",
    "    logger.info(\"Statistical areas data prepared successfully for further processing.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred during the processing of statistical areas data.\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69ed27bd-52db-4416-a006-32a46239bf6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:test_logger:Reading state geocodes data from Excel file at /dbfs/mnt/adls_custom/data/Team_B/zhastay_yeltay/zhastay_yeltay_01_bronze/state_geocodes.xlsx.\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-ea3d0f3b-e47d-440b-97b8-42a7290b28e7/lib/python3.10/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\nINFO:test_logger:Converting pandas DataFrame to Spark DataFrame and renaming columns.\nINFO:test_logger:Filtering out records where state_code is 0.\nINFO:test_logger:State geocodes data prepared successfully for further processing.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Log the reading of the state geocodes Excel file\n",
    "    logger.info(f\"Reading state geocodes data from Excel file at /{working_path.replace(':', '')}/{schema_bronze_name}/state_geocodes.xlsx.\")\n",
    "    state_geocodes = pd.read_excel(\n",
    "        f\"/{working_path.replace(':', '')}/{schema_bronze_name}/state_geocodes.xlsx\", \n",
    "        skiprows=5\n",
    "    )\n",
    "\n",
    "    # Convert pandas DataFrame to Spark DataFrame and rename columns\n",
    "    logger.info(\"Converting pandas DataFrame to Spark DataFrame and renaming columns.\")\n",
    "    bronze_state_geocodes_df = spark.createDataFrame(state_geocodes) \\\n",
    "        .withColumnsRenamed({\n",
    "            'Region': 'region',\n",
    "            'Division': 'division',\n",
    "            'State (FIPS)': 'state_code',\n",
    "            'Name': 'state'\n",
    "        })\n",
    "\n",
    "    # Log filtering of data to exclude records with state_code 0\n",
    "    logger.info(\"Filtering out records where state_code is 0.\")\n",
    "    silver_state_geocodes_df = (\n",
    "        bronze_state_geocodes_df\n",
    "        .filter(F.col('state_code') != 0)\n",
    "        .select('state_code', 'state')\n",
    "    )\n",
    "\n",
    "    # Log the completion of the data preparation\n",
    "    logger.info(\"State geocodes data prepared successfully for further processing.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred during the processing of state geocodes data.\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff5306de-adbb-4097-8eae-ee2dca9f7b9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:test_logger:Starting to join statistical areas data with state geocodes.\nINFO:test_logger:Number of metropolitan cities found: 717\nINFO:test_logger:Number of metropolitans found: 387\nINFO:test_logger:Writing metropolitan cities data to Delta table hive_metastore.zhastay_yeltay_02_silver.metropolitan_cities.\nINFO:test_logger:Metropolitan cities data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # Log the start of the data joining process\n",
    "    logger.info(\"Starting to join statistical areas data with state geocodes.\")\n",
    "    metropolitan_cities = (\n",
    "        silver_statistical_areas_df.alias('sa')\n",
    "        .join(silver_state_geocodes_df.alias('sg'), F.col('sa.state_code') == F.col('sg.state_code'), \"inner\")\n",
    "        .select('city', 'state', 'metropolitan')\n",
    "    )\n",
    "\n",
    "    # Log the number of records found (if not performance-intensive)\n",
    "    count = metropolitan_cities.count()\n",
    "    logger.info(f\"Number of metropolitan cities found: {count}\")\n",
    "\n",
    "    count = metropolitan_cities.select('metropolitan').distinct().count()\n",
    "    logger.info(f\"Number of metropolitans found: {count}\")\n",
    "\n",
    "    # Log the start of data saving operation\n",
    "    logger.info(f\"Writing metropolitan cities data to Delta table {catalog_name}.{schema_silver_name}.metropolitan_cities.\")\n",
    "    metropolitan_cities.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f'{catalog_name}.{schema_silver_name}.metropolitan_cities')\n",
    "\n",
    "    # Log the successful completion of data saving\n",
    "    logger.info(\"Metropolitan cities data saved successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred during the processing and saving of metropolitan cities data.\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18df7d22-8a67-4222-b336-b92f522a873b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# State Capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574d6a9f-ac9e-4cf9-aa9b-a6d0a2eec6b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:test_logger:Loading state capital cities data from JSON at dbfs:/mnt/adls_custom/data/Team_B/zhastay_yeltay/zhastay_yeltay_01_bronze/state_capital_cities.json.\nINFO:test_logger:Selecting city and state columns from the loaded data.\nINFO:test_logger:Writing state capital cities data to Delta table hive_metastore.zhastay_yeltay_02_silver.state_capital_cities.\nINFO:test_logger:State capital cities data saved successfully to Delta table.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Log loading JSON data\n",
    "    logger.info(\n",
    "        f\"Loading state capital cities data from JSON at {working_path}/{schema_bronze_name}/state_capital_cities.json.\"\n",
    "    )\n",
    "    bronze_state_capital_cities_df = spark.read.json(\n",
    "        f\"{working_path}/{schema_bronze_name}/state_capital_cities.json\"\n",
    "    )\n",
    "\n",
    "    state_abbreviations = {\n",
    "        \"Alabama\": \"AL\",\n",
    "        \"Alaska\": \"AK\",\n",
    "        \"Arizona\": \"AZ\",\n",
    "        \"Arkansas\": \"AR\",\n",
    "        \"California\": \"CA\",\n",
    "        \"Colorado\": \"CO\",\n",
    "        \"Connecticut\": \"CT\",\n",
    "        \"Delaware\": \"DE\",\n",
    "        \"District of Columbia\": \"DC\",\n",
    "        \"Florida\": \"FL\",\n",
    "        \"Georgia\": \"GA\",\n",
    "        \"Hawaii\": \"HI\",\n",
    "        \"Idaho\": \"ID\",\n",
    "        \"Illinois\": \"IL\",\n",
    "        \"Indiana\": \"IN\",\n",
    "        \"Iowa\": \"IA\",\n",
    "        \"Kansas\": \"KS\",\n",
    "        \"Kentucky\": \"KY\",\n",
    "        \"Louisiana\": \"LA\",\n",
    "        \"Maine\": \"ME\",\n",
    "        \"Maryland\": \"MD\",\n",
    "        \"Massachusetts\": \"MA\",\n",
    "        \"Michigan\": \"MI\",\n",
    "        \"Minnesota\": \"MN\",\n",
    "        \"Mississippi\": \"MS\",\n",
    "        \"Missouri\": \"MO\",\n",
    "        \"Montana\": \"MT\",\n",
    "        \"Nebraska\": \"NE\",\n",
    "        \"Nevada\": \"NV\",\n",
    "        \"New Hampshire\": \"NH\",\n",
    "        \"New Jersey\": \"NJ\",\n",
    "        \"New Mexico\": \"NM\",\n",
    "        \"New York\": \"NY\",\n",
    "        \"North Carolina\": \"NC\",\n",
    "        \"North Dakota\": \"ND\",\n",
    "        \"Ohio\": \"OH\",\n",
    "        \"Oklahoma\": \"OK\",\n",
    "        \"Oregon\": \"OR\",\n",
    "        \"Pennsylvania\": \"PA\",\n",
    "        \"Rhode Island\": \"RI\",\n",
    "        \"South Carolina\": \"SC\",\n",
    "        \"South Dakota\": \"SD\",\n",
    "        \"Tennessee\": \"TN\",\n",
    "        \"Texas\": \"TX\",\n",
    "        \"Utah\": \"UT\",\n",
    "        \"Vermont\": \"VT\",\n",
    "        \"Virginia\": \"VA\",\n",
    "        \"Washington\": \"WA\",\n",
    "        \"West Virginia\": \"WV\",\n",
    "        \"Wisconsin\": \"WI\",\n",
    "        \"Wyoming\": \"WY\",\n",
    "    }\n",
    "\n",
    "    states_df = spark.createDataFrame(\n",
    "        state_abbreviations.items(), [\"state_name\", \"abbreviation\"]\n",
    "    )\n",
    "\n",
    "    # Log data selection\n",
    "    logger.info(\"Selecting city and state columns from the loaded data.\")\n",
    "    silver_state_capital_cities_df = bronze_state_capital_cities_df.select(\n",
    "        \"city\", \"state\"\n",
    "    )\n",
    "\n",
    "    # Join the dataframes on the state name\n",
    "    silver_state_capital_cities_df = silver_state_capital_cities_df.join(\n",
    "        states_df, silver_state_capital_cities_df.state == states_df.state_name, \"left\"\n",
    "    ).select(\"city\", \"state\", \"abbreviation\")\n",
    "\n",
    "    # Log the start of data saving operation\n",
    "    logger.info(\n",
    "        f\"Writing state capital cities data to Delta table {catalog_name}.{schema_silver_name}.state_capital_cities.\"\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        silver_state_capital_cities_df.write.format(\"delta\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(f\"{catalog_name}.{schema_silver_name}.state_capital_cities\")\n",
    "    )\n",
    "\n",
    "    # Log successful completion of the data save operation\n",
    "    logger.info(\"State capital cities data saved successfully to Delta table.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        \"An error occurred during the processing and saving of state capital cities data.\",\n",
    "        exc_info=True,\n",
    "    )\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6cbce97-41c1-46b2-9752-6a53dd7c5502",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_states_extra_data",
   "widgets": {
    "job": {
     "currentValue": "test_logger",
     "nuid": "d33c062b-31d4-4cef-93a7-4e62ac9c3c43",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "test_logger",
      "label": null,
      "name": "job",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "task": {
     "currentValue": "test_logger",
     "nuid": "67bdfc09-ac12-419b-8f0c-c4a7933d6fed",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "test_logger",
      "label": null,
      "name": "task",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
