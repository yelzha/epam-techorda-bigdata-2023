{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e57b4a00-452a-4ff9-9f0f-6a1419648016",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('/Workspace/Repos/zhastay_yeltay@epam.com/utils/'))\n",
    "\n",
    "from delta.tables import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, DateType, LongType\n",
    "\n",
    "from init import *\n",
    "from udfs import * \n",
    "init_spark()\n",
    "\n",
    "from util_logger import init_logger\n",
    "dbutils.widgets.text('task', \"test_logger\")\n",
    "logger = init_logger(dbutils.widgets.get('task'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7abeb4cc-d4ad-4ea7-b138-2de9aeb0d3ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType, DateType\n",
    "from delta.tables import *\n",
    "\n",
    "# Assuming logger has been previously configured and imported\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    # Log loading data from the bronze layer\n",
    "    logger.info(f\"Loading orders data from the Delta table at {bronze}/orders.\")\n",
    "    bronze_orders_df = spark.read.format(\"delta\").load(f\"{bronze}/orders\")\n",
    "\n",
    "    # Log the start of data transformation and validation\n",
    "    logger.info(\"Starting transformation and validation of orders data.\")\n",
    "    silver_orders_df_upd = (\n",
    "        bronze_orders_df\n",
    "        .withColumnsRenamed({\n",
    "            \"customerId\": \"customer_id\",\n",
    "            \"createdOn\": \"created_on\",\n",
    "            \"addressId\": \"address_id\",\n",
    "            \"deliveryDate\": \"delivery_date\",\n",
    "            \"deliveredOn\": \"delivered_on\"\n",
    "        })\n",
    "        .withColumn(\n",
    "            \"is_valid\",\n",
    "            F.when(\n",
    "                F.col(\"customer_id\").isNotNull()\n",
    "                & F.col(\"created_on\").isNotNull()\n",
    "                & F.col(\"address_id\").isNotNull()\n",
    "                & F.col(\"delivery_date\").isNotNull()\n",
    "                & F.col(\"delivered_on\").isNotNull()\n",
    "                & F.col(\"id\").isNotNull()\n",
    "                & F.col(\"created_on\").between(\n",
    "                    F.lit(\"2000-01-01\").cast(TimestampType()), F.current_timestamp()\n",
    "                )\n",
    "                & F.col(\"delivery_date\").between(\n",
    "                    F.lit(\"2000-01-01\").cast(DateType()), F.current_date()\n",
    "                )\n",
    "                & F.col(\"delivered_on\").between(\n",
    "                    F.lit(\"2000-01-01\").cast(DateType()), F.current_date()\n",
    "                )\n",
    "                & (F.col(\"created_on\") <= F.col(\"delivered_on\")),\n",
    "                True,\n",
    "            ).otherwise(False),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Check for invalid entries and log the result\n",
    "    has_invalid = silver_orders_df_upd.filter(~F.col('is_valid')).limit(1).count() > 0\n",
    "    logger.info(f\"Invalid entries found: {has_invalid}\")\n",
    "\n",
    "    # Log the start of the merge operation into the silver layer\n",
    "    logger.info(\"Starting merge operation for orders data into the Silver layer.\")\n",
    "    silver_orders_df = DeltaTable.forPath(spark, f\"{silver}/orders/\")\n",
    "    silver_orders_df.alias(\"orders\").merge(\n",
    "        silver_orders_df_upd\n",
    "        .filter(F.col('is_valid'))\n",
    "        .alias(\"updates\"), \n",
    "        \"orders.id = updates.id\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\"\"\"\n",
    "                orders.customer_id != updates.customer_id\n",
    "                OR orders.created_on != updates.created_on\n",
    "                OR orders.address_id != updates.address_id\n",
    "                OR orders.delivery_date != updates.delivery_date\n",
    "                OR orders.delivered_on != updates.delivered_on\n",
    "            \"\"\",\n",
    "        set={\n",
    "            \"customer_id\": \"updates.customer_id\",\n",
    "            \"created_on\": \"updates.created_on\",\n",
    "            \"address_id\": \"updates.address_id\",\n",
    "            \"delivery_date\": \"updates.delivery_date\",\n",
    "            \"delivered_on\": \"updates.delivered_on\"\n",
    "        },\n",
    "    ).whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"customer_id\": \"updates.customer_id\",\n",
    "            \"created_on\": \"updates.created_on\",\n",
    "            \"address_id\": \"updates.address_id\",\n",
    "            \"delivery_date\": \"updates.delivery_date\",\n",
    "            \"delivered_on\": \"updates.delivered_on\",\n",
    "            \"id\": \"updates.id\"\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "    # Log successful completion of the merge operation\n",
    "    logger.info(\"Merge operation completed successfully for orders data.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred during the processing and merging of orders data.\", exc_info=True)\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1655979001493785,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_orders",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
